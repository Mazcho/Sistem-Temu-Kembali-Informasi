{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5066da",
   "metadata": {},
   "source": [
    "Preprocesing Text menggunakan NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb253e74",
   "metadata": {},
   "source": [
    "Nicholaus Verdhy || A11.2020.12447"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75aefc",
   "metadata": {},
   "source": [
    "# Penjelasan Singkat mengenai NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fb760",
   "metadata": {},
   "source": [
    "NLTK adalah kumpulan alat dan program yang digunakan untuk memproses dan menganalisis bahasa alami (NLP) dalam bahasa Inggris. Ini ditulis dalam bahasa pemrograman Python dan digunakan untuk tugas-tugas NLP yang melibatkan analisis teks dan statistik[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c46b4",
   "metadata": {},
   "source": [
    "# Project 1 : Preprocessing dengan NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4dbdb",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210ccd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#Untuk Preprocessing Data nya\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9619270",
   "metadata": {},
   "source": [
    "Penjelasan untuk setiap library\n",
    "- `import re` digunakan untuk bekerja dengan ekspresi reguler (regular expressions), yang memungkinkan Anda untuk melakukan pencarian dan manipulasi teks berdasarkan pola-pola tertentu.digunakan untuk pencarian dan manipulasi teks berdasarkan pola pola tertentu\n",
    "- `import string` digunakan dalam manipulasi teks, seperti menghapus tanda baca atau mengoperasikan karakter-karakter tertentu \n",
    "- `import time` digunakan untuk bekerja dengan waktu dan tanggal dalam Python.\n",
    "- `import pandas` digunakan untuk memuat dataset yang ada\n",
    "- `from copy import deepcopy`  ini gunanya, ketika kita melakukan duplikasi dari objek, maka hasil duplikasinya adalah objek baru dan tidak terpengaruhi oleh objek lama\n",
    "- `from ekphrasis.classes.preprocessor import TextPreProcessor` untuk melakukan berbagai tugas pemrosesan teks, terutama pada teks yang berasal dari media sosial.\n",
    "- `from ekphrasis.classes.tokenizer import SocialTokenizer`adalah tokenizer yang digunakan untuk membagi teks menjadi kata-kata\n",
    "- `from ekphrasis.dicts.emoticons import emoticons` adalah kamus yang mungkin digunakan untuk mengganti emotikon dengan kata-kata yang sesuai.\n",
    "- `import nltk` adalah pernyataan impor untuk mengimpor modul NLTK (Natural Language Toolkit), yang merupakan pustaka yang digunakan dalam pemrosesan bahasa alami (NLP)\n",
    "- `from nltk.tokenize import word_tokenize` adalah salah satu alat dalam NLTK yang digunakan untuk membagi teks menjadi kata-kata atau token. \n",
    "- `import warning` mengabaikan warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ae4d8",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060cbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset_Sentimen_Emosi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ecc78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Emosi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cegah mata rantai Covid-19,mari kita dirumah s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aku mohon yaAllah semoga wabah covid-19 menghi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pemprov Papua Naikkan Status Jadi Tanggap Daru...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covid belum nyampe prigen mbak hmm hoax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nyuruh orang pintar, lu aja Togog. Itu kerumun...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentimen  Emosi\n",
       "0  Cegah mata rantai Covid-19,mari kita dirumah s...       1.0      1\n",
       "1  aku mohon yaAllah semoga wabah covid-19 menghi...       1.0     -1\n",
       "2  Pemprov Papua Naikkan Status Jadi Tanggap Daru...       1.0      1\n",
       "3            Covid belum nyampe prigen mbak hmm hoax       0.0     -2\n",
       "4  Nyuruh orang pintar, lu aja Togog. Itu kerumun...      -1.0     -2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d94f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904 entries, 0 to 903\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Tweet     904 non-null    object \n",
      " 1   Sentimen  903 non-null    float64\n",
      " 2   Emosi     904 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 21.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eccbe3",
   "metadata": {},
   "source": [
    "Deskripsi Kolom:\n",
    "\n",
    "`Tweet` : Isi hasil tweet orang di twiter\n",
    "\n",
    "`Sentimen`  : mengindikasikan sentimen atau perasaan yang terkait dengan setiap tweet. Sentimen dapat berupa positif, negatif, netral, atau mungkin memiliki label lebih rinci seperti senang, marah, sedih, dan lain-lain.\n",
    "\n",
    "`Emosi`      : sama seperti sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579280d1",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9492424",
   "metadata": {},
   "source": [
    "Sumber modul : https://github.com/cbaziotis/ekphrasis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1bf2a",
   "metadata": {},
   "source": [
    "Modulnya ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf21aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe43eb7",
   "metadata": {},
   "source": [
    "PENJELASAN SINGKAT  TEXT_PROCESOR DARI EKPHARASIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ca426",
   "metadata": {},
   "source": [
    "Jadi **`TextPreProcessor`** adalah pustaka dari Ekpharasis. TextPreProcessing ini digunakan untuk memproses teks dalam berbagai cara sesuai dengan konfigurasi yang telah ditentukan. Teks yang diambil biasanya diambil dari media sosial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad4b8f",
   "metadata": {},
   "source": [
    "1. `normalize` : ini adalah kata kata yang dinormalisasikan dalam teks. Yang nantinya mengubah kata kata/ frasa tertentu menjadi bentuk yang lebih umum. Misal di dalam kode kita ada email, percent, money, phoinoe dan linnya. jika kita memiliki kalimat \" Nico mempuinyai email nicho@gmail.com dengan nomor hp 00000\" maka akan dinormalisasikan menjadi : `<user>`Nicho`<user>` mempunyai email `<email>`nicho@gmail.com`<email>` dengan nomor hp `<phone>` 00000`<phone>` .\n",
    "\n",
    "2. `annotate` : ini gunanya untuk mengubah bentuk yang diberi anotasi didalam kata/ kalimat. Anotasinya berupa  hastag (hastag), berupapengulaaaaaan (elonganted), KAPITALSMUA (allcaps), Woo woo woo (repeated), Amazing!!!!(Emphasis), F** and S**t (censored). hasil nya nanti seperti notmalize. Kata tersebut akan dinormalisasikan\n",
    "\n",
    "3. `fix_html` : fix html ini digunakan perbaiakan token token html dalam teks.\n",
    "\n",
    "4. `segmenter` dan `corrector` : segementer lebih untuk menyuruh ekpharasis untuk menggunakan korpus bahasa yang biasanya ada di twitter, lalu corrector melakukan koreksi ejaan yang bereferensikan dari twitter.\n",
    "\n",
    "5. `unpack_hashtags=True` : Dengan pengaturan ini, Ekphrasis akan melakukan pemisahan kata pada hashtag. dari #contoh menjadi 'contoh'\n",
    "\n",
    "6. `unpack_contractions=True` : Dengan pengaturan ini, Ekphrasis akan melakukan pemisahan pada kontraksi bahasa. Misalnya, kontraksi \"can't\" akan dipisahkan menjadi \"can\" dan \"not.\"\n",
    "\n",
    "7. `spell_correct_elong=False` : Jika diatur sebagai `True` , Ekphrasis akan melakukan koreksi ejaan pada kata-kata yang memiliki karakter pengulangan (seperti \"sooo\" menjadi \"so\"). Namun, dalam pengaturan ini (diatur sebagai False), koreksi ejaan untuk kata-kata yang diulang tidak akan dilakukan.\n",
    "\n",
    "8. `tokenizer`: Ini adalah konfigurasi untuk pemilihan tokenizer yang digunakan dalam pemrosesan teks. Mmenggunakan SocialTokenizer dengan opsi `lowercase=True`. Tokenizer ini akan **memecah teks** menjadi sejumlah token berdasarkan **spasi**, dan semua token akan **dikonversi menjadi huruf kecil (lowercase)**.\n",
    "\n",
    "9. `dicts=[emoticons]` : Ini adalah konfigurasi yang memungkinkan Anda untuk mengganti token dalam teks dengan ekspresi lain. Dalam pengaturan ini, emoticons digunakan sebagai kamus untuk mengganti emotikon dalam teks dengan ekspresi tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be984394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panggil ekphrasis\n",
    "\n",
    "def bersih_data(text):\n",
    "    return \" \".join(text_processor.pre_process_doc(text))\n",
    "\n",
    "# fungsi dari AMS 01-03.\n",
    "\n",
    "def non_ascii(text):\n",
    "    return text.encode('ascii', 'replace').decode('ascii')\n",
    "\n",
    "def remove_space_alzami(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_emoji_alzami(text):\n",
    "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\",\" \", text).split())\n",
    "\n",
    "def remove_tab(text):\n",
    "    return text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "\n",
    "def remove_tab2(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_rt(text):\n",
    "    return text.replace('RT',\" \")\n",
    "\n",
    "def remove_mention(text):\n",
    "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "\n",
    "def remove_incomplete_url(text):\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def change_stripe(text):\n",
    "    return text.replace('-',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"_\", \"\") \n",
    "    pattern = r\"[{}]\".format(remove) \n",
    "    return re.sub(pattern, \"\", text) \n",
    "\n",
    "# hapus untuk <>\n",
    "def remove_number_eks(text):\n",
    "    return text.replace('<number>',\" \")\n",
    "\n",
    "def remove_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text) \n",
    "\n",
    "def remove_URL_eks(text):\n",
    "    return text.replace('URL',\" \").replace('url',\" \")\n",
    "\n",
    "def space_punctuation(text):\n",
    "    return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4caec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d4dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd369b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e0795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e996c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa9aed0",
   "metadata": {},
   "source": [
    "# Referensi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb86798",
   "metadata": {},
   "source": [
    "[1]\tE. J. Rifano, A. C. Fauzan, A. Makhi, E. Nadya, Z. Nasikin, and F. N. Putra, “Text Summarization Menggunakan Library Natural Language Toolkit (NLTK) Berbasis Pemrograman Python,” Ilk. J. Comput. Sci. Appl. Inform., vol. 2, no. 1, Art. no. 1, Apr. 2020, doi: 10.28926/ilkomnika.v2i1.32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc29c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
