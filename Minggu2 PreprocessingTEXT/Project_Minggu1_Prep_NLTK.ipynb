{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5066da",
   "metadata": {},
   "source": [
    "Preprocesing Text menggunakan NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb253e74",
   "metadata": {},
   "source": [
    "Nicholaus Verdhy || A11.2020.12447"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75aefc",
   "metadata": {},
   "source": [
    "# Penjelasan Singkat mengenai NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fb760",
   "metadata": {},
   "source": [
    "NLTK adalah kumpulan alat dan program yang digunakan untuk memproses dan menganalisis bahasa alami (NLP) dalam bahasa Inggris. Ini ditulis dalam bahasa pemrograman Python dan digunakan untuk tugas-tugas NLP yang melibatkan analisis teks dan statistik[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c46b4",
   "metadata": {},
   "source": [
    "# Project 1 : Preprocessing dengan NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4dbdb",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210ccd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#Untuk Preprocessing Data nya\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9619270",
   "metadata": {},
   "source": [
    "Penjelasan untuk setiap library\n",
    "- `import re` digunakan untuk bekerja dengan ekspresi reguler (regular expressions), yang memungkinkan Anda untuk melakukan pencarian dan manipulasi teks berdasarkan pola-pola tertentu.digunakan untuk pencarian dan manipulasi teks berdasarkan pola pola tertentu\n",
    "- `import string` digunakan dalam manipulasi teks, seperti menghapus tanda baca atau mengoperasikan karakter-karakter tertentu \n",
    "- `import time` digunakan untuk bekerja dengan waktu dan tanggal dalam Python.\n",
    "- `import pandas` digunakan untuk memuat dataset yang ada\n",
    "- `from copy import deepcopy`  ini gunanya, ketika kita melakukan duplikasi dari objek, maka hasil duplikasinya adalah objek baru dan tidak terpengaruhi oleh objek lama\n",
    "- `from ekphrasis.classes.preprocessor import TextPreProcessor` untuk melakukan berbagai tugas pemrosesan teks, terutama pada teks yang berasal dari media sosial.\n",
    "- `from ekphrasis.classes.tokenizer import SocialTokenizer`adalah tokenizer yang digunakan untuk membagi teks menjadi kata-kata\n",
    "- `from ekphrasis.dicts.emoticons import emoticons` adalah kamus yang mungkin digunakan untuk mengganti emotikon dengan kata-kata yang sesuai.\n",
    "- `import nltk` adalah pernyataan impor untuk mengimpor modul NLTK (Natural Language Toolkit), yang merupakan pustaka yang digunakan dalam pemrosesan bahasa alami (NLP)\n",
    "- `from nltk.tokenize import word_tokenize` adalah salah satu alat dalam NLTK yang digunakan untuk membagi teks menjadi kata-kata atau token. \n",
    "- `import warning` mengabaikan warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ae4d8",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060cbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset_Sentimen_Emosi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ecc78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Emosi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cegah mata rantai Covid-19,mari kita dirumah s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aku mohon yaAllah semoga wabah covid-19 menghi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pemprov Papua Naikkan Status Jadi Tanggap Daru...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covid belum nyampe prigen mbak hmm hoax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nyuruh orang pintar, lu aja Togog. Itu kerumun...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentimen  Emosi\n",
       "0  Cegah mata rantai Covid-19,mari kita dirumah s...       1.0      1\n",
       "1  aku mohon yaAllah semoga wabah covid-19 menghi...       1.0     -1\n",
       "2  Pemprov Papua Naikkan Status Jadi Tanggap Daru...       1.0      1\n",
       "3            Covid belum nyampe prigen mbak hmm hoax       0.0     -2\n",
       "4  Nyuruh orang pintar, lu aja Togog. Itu kerumun...      -1.0     -2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d94f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904 entries, 0 to 903\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Tweet     904 non-null    object \n",
      " 1   Sentimen  903 non-null    float64\n",
      " 2   Emosi     904 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 21.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eccbe3",
   "metadata": {},
   "source": [
    "Deskripsi Kolom:\n",
    "\n",
    "`Tweet` : Isi hasil tweet orang di twiter\n",
    "\n",
    "`Sentimen`  : mengindikasikan sentimen atau perasaan yang terkait dengan setiap tweet. Sentimen dapat berupa positif, negatif, netral, atau mungkin memiliki label lebih rinci seperti senang, marah, sedih, dan lain-lain.\n",
    "\n",
    "`Emosi`      : sama seperti sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4503af5",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acd024",
   "metadata": {},
   "source": [
    "Melakukan pengecekan missing value pada dataset yang ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet       0\n",
       "Sentimen    1\n",
       "Emosi       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d25bf",
   "metadata": {},
   "source": [
    "Terdapat 1 data missing value pada kolom Sentimen, mari kita hapus dengan dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028ee242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet       0\n",
       "Sentimen    0\n",
       "Emosi       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0da367",
   "metadata": {},
   "source": [
    "Data sudah bersih, kita lanjutkan pada tahap PreProcessingg data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579280d1",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9492424",
   "metadata": {},
   "source": [
    "Sumber modul : https://github.com/cbaziotis/ekphrasis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1bf2a",
   "metadata": {},
   "source": [
    "Modulnya ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf21aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102549b",
   "metadata": {},
   "source": [
    "PENJELASAN SINGKAT  TEXT_PROCESOR DARI EKPHARASIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073bed",
   "metadata": {},
   "source": [
    "Jadi **`TextPreProcessor`** adalah pustaka dari Ekpharasis. TextPreProcessing ini digunakan untuk memproses teks dalam berbagai cara sesuai dengan konfigurasi yang telah ditentukan. Teks yang diambil biasanya diambil dari media sosial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920608d8",
   "metadata": {},
   "source": [
    "1. `normalize` : ini adalah kata kata yang dinormalisasikan dalam teks. Yang nantinya mengubah kata kata/ frasa tertentu menjadi bentuk yang lebih umum. Misal di dalam kode kita ada email, percent, money, phoinoe dan linnya. jika kita memiliki kalimat \" Nico mempuinyai email nicho@gmail.com dengan nomor hp 00000\" maka akan dinormalisasikan menjadi : `<user>`Nicho`<user>` mempunyai email `<email>`nicho@gmail.com`<email>` dengan nomor hp `<phone>` 00000`<phone>` .\n",
    "\n",
    "2. `annotate` : ini gunanya untuk mengubah bentuk yang diberi anotasi didalam kata/ kalimat. Anotasinya berupa  hastag (hastag), berupapengulaaaaaan (elonganted), KAPITALSMUA (allcaps), Woo woo woo (repeated), Amazing!!!!(Emphasis), F** and S**t (censored). hasil nya nanti seperti notmalize. Kata tersebut akan dinormalisasikan\n",
    "\n",
    "3. `fix_html` : fix html ini digunakan perbaiakan token token html dalam teks.\n",
    "\n",
    "4. `segmenter` dan `corrector` : segementer lebih untuk menyuruh ekpharasis untuk menggunakan korpus bahasa yang biasanya ada di twitter, lalu corrector melakukan koreksi ejaan yang bereferensikan dari twitter.\n",
    "\n",
    "5. `unpack_hashtags=True` : Dengan pengaturan ini, Ekphrasis akan melakukan pemisahan kata pada hashtag. dari #contoh menjadi 'contoh'\n",
    "\n",
    "6. `unpack_contractions=True` : Dengan pengaturan ini, Ekphrasis akan melakukan pemisahan pada kontraksi bahasa. Misalnya, kontraksi \"can't\" akan dipisahkan menjadi \"can\" dan \"not.\"\n",
    "\n",
    "7. `spell_correct_elong=False` : Jika diatur sebagai `True` , Ekphrasis akan melakukan koreksi ejaan pada kata-kata yang memiliki karakter pengulangan (seperti \"sooo\" menjadi \"so\"). Namun, dalam pengaturan ini (diatur sebagai False), koreksi ejaan untuk kata-kata yang diulang tidak akan dilakukan.\n",
    "\n",
    "8. `tokenizer`: Ini adalah konfigurasi untuk pemilihan tokenizer yang digunakan dalam pemrosesan teks. Mmenggunakan SocialTokenizer dengan opsi `lowercase=True`. Tokenizer ini akan **memecah teks** menjadi sejumlah token berdasarkan **spasi**, dan semua token akan **dikonversi menjadi huruf kecil (lowercase)**.\n",
    "\n",
    "9. `dicts=[emoticons]` : Ini adalah konfigurasi yang memungkinkan Anda untuk mengganti token dalam teks dengan ekspresi lain. Dalam pengaturan ini, emoticons digunakan sebagai kamus untuk mengganti emotikon dalam teks dengan ekspresi tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5c31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. memanggil ekhparasis yang sudah dibuat\n",
    "def pembersihan_data(text):\n",
    "    return \" \". join(text_processor.pre_process_doc(text))\n",
    "\n",
    "#fungsi ams 1-3\n",
    "\n",
    "#2. mengubah non ascii jadi ascii\n",
    "def mengganti_non_ascii(text):\n",
    "    return text.encode('ascii','replace').decode('ascii')\n",
    "\n",
    "#3. menghapus spasi ekstra dalam teks\n",
    "def menhapus_extra_spasi(text):\n",
    "    return \" \".join(text.split()) #dikembalikan menjadi single space, lalu dilakukan join pada text, lalu dilakukan split.\n",
    "\n",
    "#4. Menghapus emoji/ emotikon\n",
    "def menghapus_emoticon_emoji(text):\n",
    "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\",\" \", text).split())\n",
    "\n",
    "#5. menghapus karakter tab (\\t), baris baru (\\n), dan karakter khusus\n",
    "def hapus_tab(text):\n",
    "    return text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "\n",
    "#6. Menggantikan beberapa spasi berturut-turut dengan satu spasi tunggal\n",
    "def ganti_spasi_tunggal(text):\n",
    "    return re.sub('\\s+',' ',text) # jadi jika pada kalimat itu terdapat tab baru, baris baru alan digantikan dengan \" \"\n",
    "\n",
    "#7. Menghapus kata RT ( Retweet)\n",
    "def hapus_ReTweet(text):\n",
    "    return text.replace('RT',\" \") # dari kata RT, di replace jadi kosong \" \"\n",
    "\n",
    "#8 Hapus elemen elemen mention \n",
    "def hapus_mention(text):\n",
    "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split()) #dari karaketer yang ada di replace menajadi white sapce, lalau di split\n",
    "\n",
    "#9 hapus url\n",
    "def hapus_url(text):\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \") #akan menggantikan http:// dan htpps:// menjadi whitesapce\n",
    "\n",
    "#10 hapus single karakter\n",
    "def mnenghapus_single_karakter(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) # jika ada karakter huruf yang tunggal atau tidak diapit hutuf lain akan dihapus\n",
    "\n",
    "#11 hapus titik extra\n",
    "def menghapus_titik_extra(text):\n",
    "    return text.replace('..',\" \") # menghapus extra ... yang berlebihan menjadi .\n",
    "\n",
    "#12 mengahapus strip\n",
    "def mengganti_strip(text):\n",
    "    return text.replace('-',\" \") #ketika ada kalimat mengadung -, maka akan direplace dengan whitespace \" \"\n",
    "\n",
    "#13 Membuat smua huruf jadi kecil\n",
    "def huruf_kecil_semua(text):\n",
    "    return text.lower() #membuat semua teks menjadi huruf non kapital\n",
    "\n",
    "#14. Menghapus spasi di awal kalimat dan ahkir kalimat\n",
    "def hapus_spasi_awal_ahkir_kalimat(text):\n",
    "    return text.strip() #jadi strip() gunanya untuk menghapus spasi depan dan blakang\n",
    "\n",
    "#15. hapus Spasi berlebihan\n",
    "def hapus_spasi_dobel(text):\n",
    "    return re.sub('\\s+',' ',text) #menahpus spasi yang berlebihan\n",
    "\n",
    "#16. Mengapus tanda baca, kecuali tanda hubung\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"_\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    return re.sub(pattern, \"\", text) \n",
    "\n",
    "#17. hapus untuk <>\n",
    "def hapus_number_eks(text):\n",
    "    return text.replace('<number>',\" \")\n",
    "#18. hapus angka\n",
    "def hapus_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text) \n",
    "#19. Hapus teks URL\n",
    "def hapus_URL_eks(text):\n",
    "    return text.replace('URL',\" \").replace('url',\" \") #jadi teks URL atau url jadi white sapce\n",
    "#20. Pengaturan Spasi\n",
    "def pengaturan_spasi(text):\n",
    "    return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text) #memberikan spasi sebelum tanda baca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650104c",
   "metadata": {},
   "source": [
    "Penjelasan kode : \n",
    "\n",
    "1. `pembersihan_data(text)`: Menggunakan `text_processor` (yang telah diinisialisasi sebelumnya) untuk membersihkan dan memproses teks dokumen.\n",
    "\n",
    "2. `mengganti_non_ascii(text)`: Menggantikan karakter non-ASCII dengan karakter yang sesuai dalam ASCII. Karakter non-ASCII akan digantikan dengan tanda tanya atau karakter lainnya sesuai aturan penggantian.\n",
    "\n",
    "3. `menhapus_extra_spasi(text)`: Menghapus spasi ekstra dalam teks, menggantinya dengan satu spasi tunggal.\n",
    "\n",
    "4. `menghapus_emoticon_emoji(text)`: Menghapus emotikon atau emoji dalam teks dengan menggantikannya dengan spasi.\n",
    "\n",
    "5. `hapus_tab(text)`: Menghapus karakter tab (`\\t`), baris baru (`\\n`), dan karakter khusus lainnya dari teks.\n",
    "\n",
    "6. `ganti_spasi_tunggal(text)`: Menggantikan beberapa spasi berturut-turut dengan satu spasi tunggal.\n",
    "\n",
    "7. `hapus_ReTweet(text)`: Menghapus kata \"RT\" (Retweet) dari teks.\n",
    "\n",
    "8. `hapus_mention(text)`: Menghapus elemen-elemen mention atau tag dalam teks, seperti \"@username\".\n",
    "\n",
    "9. `hapus_url(text)`: Menghapus URL dari teks, baik yang dimulai dengan \"http://\" maupun \"https://\".\n",
    "\n",
    "10. `mnenghapus_single_karakter(text)`: Menghapus karakter tunggal yang tidak diapit oleh huruf lain.\n",
    "\n",
    "11. `menghapus_titik_extra(text)`: Menghapus tanda titik yang berlebihan dan menggantikannya dengan spasi.\n",
    "\n",
    "12. `mengganti_strip(text)`: Menggantikan tanda strip (-) dengan spasi.\n",
    "\n",
    "13. `huruf_kecil_semua(text)`: Mengubah semua huruf dalam teks menjadi huruf kecil.\n",
    "\n",
    "14. `hapus_spasi_awal_ahkir_kalimat(text)`: Menghapus spasi di awal dan akhir kalimat.\n",
    "\n",
    "15. `hapus_spasi_dobel(text)`: Menghapus spasi berlebihan dalam teks.\n",
    "\n",
    "16. `remove_punctuation(text)`: Menghapus tanda baca, kecuali tanda hubung (-).\n",
    "\n",
    "17. `hapus_number_eks(text)`: Menghapus teks \"<number>\".\n",
    "\n",
    "18. `hapus_angka(text)`: Menghapus semua angka dalam teks.\n",
    "\n",
    "19. `hapus_URL_eks(text)`: Menghapus teks \"URL\" atau \"url\".\n",
    "\n",
    "20. `pengaturan_spasi(text)`: Menambahkan spasi sebelum tanda baca, menjaga spasi di antara kata-kata dan tanda baca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7d20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af70439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa9aed0",
   "metadata": {},
   "source": [
    "# Referensi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb86798",
   "metadata": {},
   "source": [
    "[1]\tE. J. Rifano, A. C. Fauzan, A. Makhi, E. Nadya, Z. Nasikin, and F. N. Putra, “Text Summarization Menggunakan Library Natural Language Toolkit (NLTK) Berbasis Pemrograman Python,” Ilk. J. Comput. Sci. Appl. Inform., vol. 2, no. 1, Art. no. 1, Apr. 2020, doi: 10.28926/ilkomnika.v2i1.32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc29c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
